info:
  title: OOPSLA
  address: '22, Soonchunhyang-ro, Sinchang-myeon, Asan-si, Chungcheongnam-do'
  office: 'Dept. of Computer Science & Engineering, SCH University.'
  email: 'hsh@sch.ac.kr' # contact us

nav:
  - - news
    - '/news'
  - - course
    - '/course'
  - - research
    - '/research'
  - - people
    - '/people'
  - - alumni
    - '/alumni'

news:
  -
    date: '2019-12-08'
    thumbnail: 'http://cvlab.postech.ac.kr/lab/images/news/acml2019.png'
    title: 'Our paper about regularizing neural networks has been accepted to ACML 2019.'
    contents: '1 ㅔㅌ스트 콘텐츠'
  -
    date: '2019-11-5'
    thumbnail: 'http://cvlab.postech.ac.kr/lab/images/news/NeurIPS.png'
    title: 'Two papers have been accepted to NeurIPS 2019.'
    contents: '22'
  -
    date: '2019-12-10'
    thumbnail: 'http://cvlab.postech.ac.kr/lab/images/news/IJCV.jpg'
    title: 'Prof. Cho has joined the editorial board of International Journal of Computer Vision (IJCV) as an associate editor.'
    contents: '1 ㅔㅌ스트 콘텐츠'
  -
    date: '2019-12-21'
    thumbnail: 'http://cvlab.postech.ac.kr/lab/images/news/aaai20.png'
    title: 'Three papers have been accepted to AAAI 2020.'
    contents: 'etwet ccc'

research:
  -
    title: 'MotionCapture: Neural Motion Feature Learning for Video Understanding'
    thumbnail: 'http://cvlab.postech.ac.kr/research/MoCap/images/MoCap.png'
    writer:
      - 'Heeseung Kwon'
      - 'Manjin Kim'
      - 'Suha Kwak'
    Abstract: 'Motion plays a crucial role in understanding videos and thus most state-of-the-art neural models for video classification incorporate motion information typically by extracting optical flows frame-by-frame using a separate off-the-shelf method. As the optical flows require heavy computation, incorporating motion information has remained a main computational bottleneck for video understanding. In this work, we attempt to replace the external and heavy computation of optical flows with an internal and light-weight learning of motion features. The proposed method, dubbed the MotionCapture (MoCap) module, is an end-to-end trainable block for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate the effectiveness of our method on three standard benchmarks for video action recognition, where inserting the proposed module achieves the state-of-the-art with only a small amount of additional cost.'
    Overall architecture: >
      <img src="http://cvlab.postech.ac.kr/research/MoCap/images/overall_arch.png">
      Figure 1. Overall architecture of the proposed approach. The model first takes multiple video frames as input and convert them into frame-wise appearance features using convolutional layers. The proposed MotionCapture (MoCap) module generates motion features using the frame-wise appearance features, and combines the motion features into the next downstream layer.
    papers:
      -
        title: 'MotionCapture: Neural Motion Feature Learning for Video Understanding'
        writer:
          - 'Heeseung Kwon'
          - 'Manjin Kim'
          - 'Suha Kwak'
          - 'Minsu Cho'
        links:
          - - 'arXiv, 2019'
            - 'www.google.com'
          - - 'Bibtex, 2019'
  -
    title: 'SPair-71k: A Large-scale Benchmark for Semantic Correspondence'
    thumbnail: 'http://cvlab.postech.ac.kr/research/SPair-71k/images/SPair_teaser.png'
    writer:
      - 'Juhong Min'
      - 'Jongmin Lee'
      - 'Jean Ponce'
    abstract: 'Establishing visual correspondences under large intra-class variations, which is often referred to as semantic correspondence or semantic matching, remains a challenging problem in computer vision. Despite its significance, however, most of the datasets for semantic correspondence are limited to a small amount of image pairs with similar viewpoints and scales. In this paper, we present a new large-scale benchmark dataset of semantically paired images, SPair-71k, which contains 70,958 image pairs with diverse variations in viewpoint and scale. Compared to previous datasets, it is significantly larger in number and contains more accurate and richer annotations. We believe this dataset will provide a reliable testbed to study the problem of semantic correspondence and will help to advance research in this area. We provide the results of recent methods on our new dataset as baselines for further research.'
    sdsdf: 'testest'
    papers:
      -
        title: 'Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features'
        writer:
          - 'Juhong Min'
          - 'Jongmin Lee'
          - 'Jean Ponce'
        links:
          - - 'arXiv'
  -
    title: 'Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features'
    thumbnail: 'http://cvlab.postech.ac.kr/research/HPF/images/architecture.png'
    writer:
      - 'Juhong Min'
      - 'Jongmin Lee'
      - 'Jean Ponce'
    abs: 'testtest'
    sdsdf: 'testest'
    papers:
      -
        title: 'paper title 1'
        writer:
          - 'writer1'
          - 'writer 2'
        links:
          - - 'arXiv'
            - 'www.google.com'
          - - 'Bibtex'
            - 'www.naver.com'
  -
    title: 'Relational Knowledge Distillation'
    thumbnail: 'http://cvlab.postech.ac.kr/research/RKD/images/rkd.png'
    writer:
      - 'Wonpyo Park'
      - 'Dongju Kim'
    abs: 'testtest'
    sdsdf: 'testest'
    papers:
      -
        title: 'paper title 1'
        writer:
          - 'writer1'
          - 'writer 2'
        links:
          - - 'arXiv'
            - 'www.google.com'
          - - 'Bibtex'
            - 'www.naver.com'

course:
  -
    title:
      - '데이터 구조 2'
      - 'Data Structures 2'
    link: 'google.com'
  -
    title:
      - '객체지향프로그래밍 & Lab'
      - 'OOP & Lab'
    link: 'google.com'

people:
  professor: # customize-able
    -
      name:
        - '하 상호'
        - 'SangHo Ha'
      email: 'hsh@sch.ac.kr'
      subject: 'test subject'
      thumbnail: '/professor.png'
      detail: # detail page
        office: 'Department of Computer Science & Engineering, Soonchunhyang University (순천향대학교)'
        address: '22, Soonchunhyang-ro, Sinchang-myeon, Asan-si, Chungcheongnam-do, Republic of Korea (31538)'
        tel: '+82 041-530-1279'
        fax: '+82 041-630-1548'
        contents:
          cus: 'testtest' # customizable
          listitme:
            - '11'
            - 'testtesttewt222'
            - 'tewtwtwetwet333'
          canusinghtml:
            - '<b>test</b>tset [<a href="www.google.com">PDF</a>] test test'
            - '<b>test</b>tset [<a href="www.google.com">PDF</a>] test test'
  B. Students:
    -
      name:
        - '한글을 좋아함 그래서 영어이름이 없음'
      email: 'gds@gmail.com'
      subject: 'u-Health, Chest-movement'
    -
      name:
        - '한글'
        - 'English'
      email: 'dfs@gmail.com'
      subject: 'u-Health, Chest-movement'
      thumbnail: '/student-2.png'

alumni:
  -
    name:
      - '그대로 옮김'
      - 'English'
    email: 'dfs@gmail.com'
    subject: 'u-Health, Chest-movement'
    thumbnail: '/student-2.png'
